
### Introduction
Watch a quick introduction to Laktory, the open-source ETL framework, and learn how you can leverage its pipeline model 
to efficiently build and deploy dataframe-centric pipelines to Databricks or other data platforms.

![type:video](https://www.youtube.com/embed/BZK0PE-OriQ)

### Lakehouse As Code
A mini-series about setting up and end-to-end Databricks Lakehouse using Laktory.

1. **Unity Catalog**: Setting the foundation of a Unity Catalog, including users management, schemas and volumes. 
    ![type:video](https://www.youtube.com/embed/G2Ol0bME5h0)

2. **Workspace**: Configuring a Workspace with  clusters, warehouses and secrets
    ![type:video](https://www.youtube.com/embed/nwsyS2SU2mw)

3. **Pipeline Job**: Declaring a Laktory pipeline and deploying it as a Databricks Job.
    ![type:video](https://www.youtube.com/embed/dyArq_CUqKc)

4. **Delta Live Tables**: Declaring a Laktory pipeline and deploying it as a Delta Live Tables 
    ![type:video](https://www.youtube.com/embed/cX3EPV_xWrM)

5. **AI/BI Dashboard**: How to easily create an AI/BI Dashboard and deploy it to multiple workspaces.

### Overview
Watch a hands-on demo on how to use Laktory to build a scalable data pipeline. It covers:

- Declare a data pipeline backed by Polars dataframes and run it locally
- Change backend to Spark dataframe and run it on a Databricks cluster
- Deploy as a databricks job and as a Delta Live Tables using Laktory CLI

![type:video](https://www.youtube.com/embed/010w2iWrN0w)


### Requests
You may request a live demo from Okube [website](https://www.okube.ai/contact).