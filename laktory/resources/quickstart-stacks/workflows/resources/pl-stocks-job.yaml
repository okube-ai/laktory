#
# This is an example of a pipeline that uses Databricks Jobs as the
# orchestrator. It runs in batch mode (as_stream = `False`), meaning
# that each table is re-computed entirely at each run.
#
name: pl-stocks-job

# Configure Orchestrator
orchestrator:
  type: DATABRICKS_JOB
# Uncomment to use job cluster instead of serverless
#  job_clusters:
#    - job_cluster_key: node-cluster
#      new_cluster:
#        autoscale:
#          min_workers: 1
#          max_workers: 2
#        spark_version: 16.4.x-scala2.13
#        node_type_id: Standard_DS3_v2

dependencies:
  - laktory==<laktory_version>
  - ${vars.wheel_filepath}

# Define nodes (each node is a table or a view)
nodes:

# Bronze Table
- name: brz_stock_prices
  source:
    path: dbfs:/laktory/data/stock_prices/
    as_stream: false
    format: JSONL
  sinks:
  - table_name: brz_stock_prices_job
    mode: OVERWRITE

# Silver Table
- name: slv_stock_prices
  expectations:
  - name: positive_price
    expr: open > 0
    action: DROP
  source:
    node_name: brz_stock_prices
    as_stream: false
  sinks:
  - table_name: slv_stock_prices_job
    mode: OVERWRITE

  # The transformer is a chain of SQL statements and / or serialized spark
  # functions. In this case, we use SQL to select some columns from the
  # source and we use pyspark df.drop_duplicates to clean the data. Because the API
  # used by default by Laktory is Narwhals, we need to specific `dataframe_api: NATIVE`
  # to directly use spark API
  transformer:
    nodes:
    - expr: !use ../sql/slv_stock_prices.sql
    - func_name: drop_duplicates
      func_kwargs:
        subset: ["created_at", "symbol"]
      dataframe_api: NATIVE

    # Custom function from lake package deployed as wheel file
    - func_name: lake.with_last_modified