#
# This is an example of a pipeline that uses Delta Live Tables as the
# orchestrator. It runs in stream mode (as_stream = `True`), meaning that
# new files are incrementally pushed downstream as new rows instead of
# re-computing entirely the downstream tables.
#
name: pl-stocks-dlt

# Configure Orchestrator
orchestrator:
  type: DATABRICKS_DLT
  development: ${vars.is_dev}
  target: default
  clusters:
  - name: default
    node_type_id: Standard_DS3_v2  # TODO: Change to your desired cloud-specific node type
    autoscale:
      min_workers: 1
      max_workers: 2
  libraries:
  - notebook:
      path: /.laktory/dlt/dlt_laktory_pl.py

# Define nodes (each node is a table or a view)
nodes:

# Bronze Table
- name: brz_stock_prices
  source:
    path: dbfs:/laktory/data/stock_prices/
    as_stream: true
    format: JSONL
  sinks:
  - table_name: brz_stock_prices

# Silver Table
- name: slv_stock_prices
  expectations:
  - name: positive_price
    expr: open > 0
    action: FAIL
  source:
    node_name: brz_stock_prices
    as_stream: true
  sinks:
  - table_name: slv_stock_prices

  # The transformer is a chain of SQL statements and / or serialized spark
  # functions. In this case, we use `with_columns` from Narwhals API and the
  # drop_duplicates method from Spark. By default, laktory will use Narwhals API to
  # ensure seamless transition between DataFrame backends, but you can also choose to
  # use DataFrame backend API by setting `dataframe_api` to "NATIVE".
  transformer:
    nodes:
    - func_name: withColumns
      func_kwargs:
        created_at: nw.col("data").field("created_at).cast(nw.dtypes.Timestamp())
        symbol: nw.col("data").struct.field("symbol").cast(nw.dtypes.String())
        open: nw.col("data").struct.field("open).cast(nw.dtypes.Float64())
        close: nw.col("data").struct.field("close).cast(nw.dtypes.Float64())
    - func_name: drop_duplicates
      func_kwargs:
        subset: ["created_at", "symbol"]
      dataframe_api: NATIVE