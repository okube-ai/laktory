name: workflows
description: Lakehouse - Workflows
backend: terraform

# List of Resources to deploy
resources:

  # Python package (lake)
  databricks_pythonpackages: !use resources/pythonpackages.yaml

  # Notebooks for jobs and pipelines
  databricks_workspacetrees:
    workspacetree-0:
      source: ./workspacefiles/

  # Simple sample job
  databricks_jobs:
    job-hello: !use resources/job-hello.yaml

  # Laktory Sample Pipelines
  #   - Deployed resources depend on selected orchestrator
  #   - pl-stocks-job: Deployed as a Job with SQL transformations
  #   - pl-stocks-dlt: Deployed as a Lakeflow Declarative Pipeline with spark transformations
  pipelines:
    pl-stocks-job: !use resources/pl-stocks-job.yaml
    pl-stocks-dlt: !use resources/pl-stocks-dlt.yaml

  # Databricks Provider Configuration
  providers:
    databricks:
      host: ${vars.DATABRICKS_HOST}
      token: ${vars.DATABRICKS_TOKEN}

  variables:
    wheel_filepath: /Workspace${vars.workspace_laktory_root}wheels/lake-0.0.1-py3-none-any.whl

# Environment Specific Settings
environments:

  dev:
    variables:
      env: dev
      is_dev: true

  prd:
    variables:
      env: prd
      is_dev: false
